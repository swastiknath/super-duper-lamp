# Heading towards the power of OLAP:

### Problem Statement:
A music streaming startup, Sparkify, has grown their user base and song database and they wanted to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

In this task an ETL pipeline that extracts the data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to and what are the aspects that will keep their users glued to their application. 

From the Data Engineering aspects, as the userbase of the app is continually increasing, the process of the ETL should be scalable and the analytical workloads for the analysts should be simpler to increase efficiencies of the analysts to uncover hidden insights of the data. 

### Harnessing the Power of AWS and Cloud:

With the analytical power of Redshift, Sparkify will be able to get even more insights out of their data. AWS has automated the process to scale the processing resources and storage capacity when needed and they need to pay for what they use. Redshift and S3 is also capable of processing massive capacity of data with minimal time.

For the database design, the star schema approach will be kept for this new strategy. The star schema has its strenghs for business dimensions and will be able to provide the best analytical model to their analysts. 

### Explaining the Files:

In the **/data** folder we have got another two directories which are **song_data** and the **log_data**. 

##### song_data: 
The first dataset contained in the **song_data** folder is a subset of real data from the Million Song Dataset. Each file is 
in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three 
letters of each song's track ID. For example, here are filepaths to two files in this dataset which conforms to the directory 
structure of these dataset.
```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```
An instance of the data file inside these datasets are: 
```
{"num_songs": 1, "artist_id": "ARI2JSK1187FB496EF", "artist_latitude": 51.50632, "artist_longitude": -0.12714, "artist_location": "London, England", "artist_name": "Nick Ingman;Gavyn Wright", "song_id": "SODUJBS12A8C132150", "title": "Wessex Loses a Bride", "duration": 111.62077, "year": 0}
```

##### log_data:
The second dataset contained in the **logs_data** is of log files in JSON format generated by an [event simulator]('https://github.com/Interana/eventsim') based on the songs in the dataset above. These simulate activity logs from 
a music streaming app based on specified configurations.

The log files in the dataset we are working with are partitioned by year and month. For example, filepaths to two 
files in this dataset.

```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```
An instance of the data file inside these datasets are:
```
{"artist":null,"auth":"Logged In","firstName":"Walter","gender":"M","itemInSession":0,"lastName":"Frye","length":null,"level":"free","location":"San Francisco-Oakland-Hayward, CA","method":"GET","page":"Home","registration":1540919166796.0,"sessionId":38,"song":null,"status":200,"ts":1541105830796,"userAgent":"\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"","userId":"39"}
```

### Usage Instructions:
Create a Configuration file `dwh.cfg` with the following format and fill it in with the details about the Redshift instance details and S3 source.
```
[CLUSTER]
HOST='####'
DB_NAME='####'
DB_USER='####'
DB_PASSWORD='####'
DB_PORT='####'

[IAM_ROLE]
ARN="#####"

[S3]
LOG_DATA='#####'
LOG_JSONPATH='####'
SONG_DATA='####'
```
This package containes a total of 3 following python scripts:
     
  1. **sql_queries.py** : Containes the SQL instructions for creating, inserting, modifying and dropping the schemas and tables for both of the staging                             tables and fact and dimension tables.
  2. **create_tables.py**: Containes various functions to call the SQL instructions and creating the schemas and tables. It goes on and drop any previous                            instance of tables. 
  3. **etl.py** :          Containes various functions to call the SQL instructions and extracting the values from the staging table then process and                                transforms it and then loads them into the fact and dimensional tables.
  
To get started issue the following commands from the **Terminal**:
```
python create_tables.py
python etl.py
```